# Ollama Configuration
# The base URL where your Ollama server is running
OLLAMA_BASE_URL=http://localhost:11434/v1

# The Ollama model to use for processing chrome tab content
# Examples: llama2, qwen, mistral, etc.
OLLAMA_MODEL=llama2

# Optional: Context length (num_ctx) for the Ollama model
# Controls the context window size in tokens
# Default varies by model (typically 2048)
# Increase this for larger documents or longer conversations
# Example values: 4096, 8192, 16384, 32768
# OLLAMA_CONTEXT_LENGTH=8192

# Optional: Authentication token for the native messaging bridge
# Only needed if the native host is started with --require-auth
# BRIDGE_AUTH_TOKEN=your-secret-token-here

# Optional: Exclude URLs from logs (comma-separated patterns)
# Useful to reduce noise from test URLs in logs
# Example: CHROME_TAB_LOG_EXCLUDE_URLS=example.com,test.local,localhost:3000
# CHROME_TAB_LOG_EXCLUDE_URLS=
