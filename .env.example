# Ollama Configuration
# The base URL where your Ollama server is running
OLLAMA_BASE_URL=http://localhost:11434/v1

# The Ollama model to use for processing chrome tab content
# Examples: llama2, qwen, mistral, etc.
OLLAMA_MODEL=llama2

# Optional: Authentication token for the native messaging bridge
# Only needed if the native host is started with --require-auth
# BRIDGE_AUTH_TOKEN=your-secret-token-here
